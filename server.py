from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import httpx
import os
from dotenv import load_dotenv
from typing import List

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

app = FastAPI(title="RAPTOR API Server", version="1.0.0")

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000", "http://localhost:3001", "http://localhost", "http://localhost:80"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Pydantic ëª¨ë¸ë“¤
class ChatMessage(BaseModel):
    role: str
    content: str

class ChatRequest(BaseModel):
    messages: List[ChatMessage]

class ChatResponse(BaseModel):
    content: List[dict]
    id: str
    model: str
    role: str
    stop_reason: str
    usage: dict

@app.get("/")
async def root():
    return {"message": "ğŸ¦– RAPTOR API Server is running!"}

@app.post("/api/claude/messages", response_model=dict)
async def proxy_ollama_api(request: ChatRequest):
    """Ollama API í”„ë¡ì‹œ ì—”ë“œí¬ì¸íŠ¸"""
    
    # Ollama ì„œë²„ URL (ê¸°ë³¸ê°’: localhost:11434)
    ollama_url = os.getenv("OLLAMA_URL", "http://localhost:11434")
    model = os.getenv("OLLAMA_MODEL", "deepseek-r1:1.5b")
    
    # Ollama API í˜¸ì¶œì„ ìœ„í•œ ë©”ì‹œì§€ í˜•ì‹ ë³€í™˜
    messages = []
    system_message = "You are RAPTOR, an AI assistant specialized in root cause analysis and causal relationship identification. You help users understand complex systems through ontology-based reasoning to identify root causes of problems."
    
    # ì‹œìŠ¤í…œ ë©”ì‹œì§€ë¥¼ ì²« ë²ˆì§¸ ë©”ì‹œì§€ë¡œ ì¶”ê°€
    messages.append({"role": "system", "content": system_message})
    
    # ì‚¬ìš©ì ë©”ì‹œì§€ë“¤ ì¶”ê°€
    for msg in request.messages:
        messages.append({"role": msg.role, "content": msg.content})
    
    payload = {
        "model": model,
        "messages": messages,
        "stream": False
    }
    
    try:
        async with httpx.AsyncClient() as client:
            response = await client.post(
                f"{ollama_url}/api/chat",
                json=payload,
                timeout=60.0
            )
            
            if response.status_code != 200:
                raise HTTPException(
                    status_code=response.status_code,
                    detail=f"Ollama API error: {response.text}"
                )
            
            ollama_response = response.json()
            
            # Claude API í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ ë³€í™˜
            claude_format_response = {
                "content": [{"text": ollama_response["message"]["content"]}],
                "id": "ollama-" + str(hash(ollama_response["message"]["content"]))[:10],
                "model": model,
                "role": "assistant",
                "stop_reason": "end_turn",
                "usage": {
                    "input_tokens": 0,
                    "output_tokens": 0
                }
            }
            
            return claude_format_response
            
    except httpx.TimeoutException:
        raise HTTPException(status_code=408, detail="Request to Ollama API timed out")
    except httpx.RequestError as e:
        raise HTTPException(status_code=502, detail=f"Failed to connect to Ollama API: {str(e)}")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Internal server error: {str(e)}")

if __name__ == "__main__":
    import uvicorn
    uvicorn.run("server:app", host="0.0.0.0", port=8000, reload=True)